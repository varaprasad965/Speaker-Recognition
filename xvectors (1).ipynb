{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxZ_7zLDPdRr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3c_sJNAO1zZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "orig_dir = os.getcwd()\n",
        "os.chdir('/content/drive/MyDrive')\n",
        "from mfcc import MFCC\n",
        "os.chdir(orig_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyn-mpJaqUmb",
        "outputId": "07deda65-d585-4671-9cfd-347f207ed6fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUGPB7cXPE9f"
      },
      "outputs": [],
      "source": [
        "def extract_features(file_path):\n",
        "    mfccs = MFCC(file_path)\n",
        "    return mfccs.T\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/SPEECH_A6/sre_new_dataset'\n",
        "speaker_dirs = os.listdir(data_dir)\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "for label, speaker_dir in enumerate(speaker_dirs):\n",
        "    speaker_path = os.path.join(data_dir, speaker_dir)\n",
        "    for wav_file in os.listdir(speaker_path):\n",
        "        file_path = os.path.join(speaker_path, wav_file)\n",
        "        features.append(extract_features(file_path))\n",
        "        labels.append(label)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_speakers = 100\n",
        "slice_dim = 400\n",
        "output_dim = num_speakers"
      ],
      "metadata": {
        "id": "NkHXVm78TxNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_features = []\n",
        "split_labels = []\n",
        "for i in range(len(features)):\n",
        "  for j in range(0, len(features[i]), slice_dim):\n",
        "    if j + slice_dim < len(features[i]):\n",
        "      split_features.append(features[i][j:j+slice_dim])\n",
        "      split_labels.append(labels[i])"
      ],
      "metadata": {
        "id": "Jad6kM2BWUHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(split_features, split_labels, test_size=0.2)\n",
        "train_features, test_features, train_labels, test_labels = torch.tensor(train_features).to(device), torch.tensor(test_features).to(device), torch.tensor(train_labels).to(device), torch.tensor(test_labels).to(device)\n",
        "print(len(split_features), len(train_features), len(test_features), len(train_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4u2hIxiW2Jn",
        "outputId": "cdc64f2e-7691-4380-982d-e9772e491c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-dbfb997c433b>:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  train_features, test_features, train_labels, test_labels = torch.tensor(train_features).to(device), torch.tensor(test_features).to(device), torch.tensor(train_labels).to(device), torch.tensor(test_labels).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7857 6285 1572 6285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(train_features)):\n",
        "  if train_features[i].shape[0] != slice_dim:\n",
        "    print('Error')"
      ],
      "metadata": {
        "id": "bBedUavyW_gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLYnuHxIPjgX"
      },
      "outputs": [],
      "source": [
        "class SpeakerDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        mfccs = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return mfccs, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6ITNsPmOyYd"
      },
      "outputs": [],
      "source": [
        "class TDNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(TDNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=128, kernel_size=9, stride=1, padding = 4)\n",
        "        self.conv2 = nn.Conv1d(in_channels=128, out_channels= 256, kernel_size=7, stride=1, padding = 3)\n",
        "        self.conv3 = nn.Conv1d(in_channels=256, out_channels= 256, kernel_size=5, stride=1, padding = 2)\n",
        "        self.conv4 = nn.Conv1d(in_channels=256, out_channels= 256, kernel_size=3, stride=1, padding = 1)\n",
        "        self.conv5 = nn.Conv1d(in_channels=256, out_channels=output_dim, kernel_size=1, stride=1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.conv5(x)\n",
        "\n",
        "        x = torch.mean(x, dim=-1)\n",
        "\n",
        "        return x\n",
        "\n",
        "train_dataset = SpeakerDataset(train_features, train_labels)\n",
        "test_dataset = SpeakerDataset(test_features, test_labels)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 42\n",
        "model = TDNN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
        "\n",
        "# Define the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 85\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    if epoch > 40:\n",
        "        optimizer.param_groups[0]['lr'] = 5e-5\n",
        "    if epoch > 55:\n",
        "        optimizer.param_groups[0]['lr'] = 3e-5\n",
        "    if epoch > 70:\n",
        "        optimizer.param_groups[0]['lr'] = 1e-5\n",
        "\n",
        "    # optimizer.param_groups[0]['lr'] *= 0.99\n",
        "    # if epoch > 25:\n",
        "    #   optimizer.param_groups[0]['lr'] *= 0.98\n",
        "    # if epoch > 50:\n",
        "    #   optimizer.param_groups[0]['lr'] *= 0.97\n",
        "\n",
        "    for batch_idx, (mfccs, targets) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        mfccs= mfccs.permute(0, 2, 1)\n",
        "        outputs = model(mfccs.float())\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (mfccs, targets) in enumerate(test_loader):\n",
        "            mfccs= mfccs.permute(0, 2, 1)\n",
        "            outputs = model(mfccs.float())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            predicted = predicted.reshape(-1,)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Validation Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPOiOvuGYJch",
        "outputId": "d9c59419-993b-4473-9702-a3cdb7abe5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/85 - Validation Accuracy: 1.97%\n",
            "Epoch 2/85 - Validation Accuracy: 1.72%\n",
            "Epoch 3/85 - Validation Accuracy: 3.63%\n",
            "Epoch 4/85 - Validation Accuracy: 4.33%\n",
            "Epoch 5/85 - Validation Accuracy: 4.39%\n",
            "Epoch 6/85 - Validation Accuracy: 4.71%\n",
            "Epoch 7/85 - Validation Accuracy: 5.03%\n",
            "Epoch 8/85 - Validation Accuracy: 6.23%\n",
            "Epoch 9/85 - Validation Accuracy: 7.57%\n",
            "Epoch 10/85 - Validation Accuracy: 8.59%\n",
            "Epoch 11/85 - Validation Accuracy: 9.80%\n",
            "Epoch 12/85 - Validation Accuracy: 14.31%\n",
            "Epoch 13/85 - Validation Accuracy: 13.61%\n",
            "Epoch 14/85 - Validation Accuracy: 18.45%\n",
            "Epoch 15/85 - Validation Accuracy: 20.04%\n",
            "Epoch 16/85 - Validation Accuracy: 19.91%\n",
            "Epoch 17/85 - Validation Accuracy: 24.36%\n",
            "Epoch 18/85 - Validation Accuracy: 32.12%\n",
            "Epoch 19/85 - Validation Accuracy: 32.19%\n",
            "Epoch 20/85 - Validation Accuracy: 36.26%\n",
            "Epoch 21/85 - Validation Accuracy: 39.82%\n",
            "Epoch 22/85 - Validation Accuracy: 38.68%\n",
            "Epoch 23/85 - Validation Accuracy: 45.42%\n",
            "Epoch 24/85 - Validation Accuracy: 47.52%\n",
            "Epoch 25/85 - Validation Accuracy: 44.34%\n",
            "Epoch 26/85 - Validation Accuracy: 49.94%\n",
            "Epoch 27/85 - Validation Accuracy: 46.95%\n",
            "Epoch 28/85 - Validation Accuracy: 51.02%\n",
            "Epoch 29/85 - Validation Accuracy: 47.96%\n",
            "Epoch 30/85 - Validation Accuracy: 49.24%\n",
            "Epoch 31/85 - Validation Accuracy: 50.13%\n",
            "Epoch 32/85 - Validation Accuracy: 51.27%\n",
            "Epoch 33/85 - Validation Accuracy: 56.49%\n",
            "Epoch 34/85 - Validation Accuracy: 54.90%\n",
            "Epoch 35/85 - Validation Accuracy: 58.02%\n",
            "Epoch 36/85 - Validation Accuracy: 54.96%\n",
            "Epoch 37/85 - Validation Accuracy: 63.10%\n",
            "Epoch 38/85 - Validation Accuracy: 61.70%\n",
            "Epoch 39/85 - Validation Accuracy: 55.92%\n",
            "Epoch 40/85 - Validation Accuracy: 61.13%\n",
            "Epoch 41/85 - Validation Accuracy: 65.46%\n",
            "Epoch 42/85 - Validation Accuracy: 71.88%\n",
            "Epoch 43/85 - Validation Accuracy: 72.90%\n",
            "Epoch 44/85 - Validation Accuracy: 71.76%\n",
            "Epoch 45/85 - Validation Accuracy: 68.00%\n",
            "Epoch 46/85 - Validation Accuracy: 69.34%\n",
            "Epoch 47/85 - Validation Accuracy: 66.16%\n",
            "Epoch 48/85 - Validation Accuracy: 67.24%\n",
            "Epoch 49/85 - Validation Accuracy: 67.11%\n",
            "Epoch 50/85 - Validation Accuracy: 65.90%\n",
            "Epoch 51/85 - Validation Accuracy: 68.19%\n",
            "Epoch 52/85 - Validation Accuracy: 69.78%\n",
            "Epoch 53/85 - Validation Accuracy: 71.88%\n",
            "Epoch 54/85 - Validation Accuracy: 72.20%\n",
            "Epoch 55/85 - Validation Accuracy: 61.51%\n",
            "Epoch 56/85 - Validation Accuracy: 69.85%\n",
            "Epoch 57/85 - Validation Accuracy: 77.04%\n",
            "Epoch 58/85 - Validation Accuracy: 76.97%\n",
            "Epoch 59/85 - Validation Accuracy: 76.40%\n",
            "Epoch 60/85 - Validation Accuracy: 73.73%\n",
            "Epoch 61/85 - Validation Accuracy: 77.16%\n",
            "Epoch 62/85 - Validation Accuracy: 73.85%\n",
            "Epoch 63/85 - Validation Accuracy: 76.08%\n",
            "Epoch 64/85 - Validation Accuracy: 75.51%\n",
            "Epoch 65/85 - Validation Accuracy: 77.86%\n",
            "Epoch 66/85 - Validation Accuracy: 75.95%\n",
            "Epoch 67/85 - Validation Accuracy: 65.84%\n",
            "Epoch 68/85 - Validation Accuracy: 76.08%\n",
            "Epoch 69/85 - Validation Accuracy: 76.34%\n",
            "Epoch 70/85 - Validation Accuracy: 76.72%\n",
            "Epoch 71/85 - Validation Accuracy: 77.10%\n",
            "Epoch 72/85 - Validation Accuracy: 77.42%\n",
            "Epoch 73/85 - Validation Accuracy: 77.54%\n",
            "Epoch 74/85 - Validation Accuracy: 77.61%\n",
            "Epoch 75/85 - Validation Accuracy: 77.61%\n",
            "Epoch 76/85 - Validation Accuracy: 77.74%\n",
            "Epoch 77/85 - Validation Accuracy: 77.61%\n",
            "Epoch 78/85 - Validation Accuracy: 76.91%\n",
            "Epoch 79/85 - Validation Accuracy: 77.67%\n",
            "Epoch 80/85 - Validation Accuracy: 77.29%\n",
            "Epoch 81/85 - Validation Accuracy: 77.48%\n",
            "Epoch 82/85 - Validation Accuracy: 77.29%\n",
            "Epoch 83/85 - Validation Accuracy: 76.53%\n",
            "Epoch 84/85 - Validation Accuracy: 77.29%\n",
            "Epoch 85/85 - Validation Accuracy: 78.18%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X8fzdnTZYn4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}